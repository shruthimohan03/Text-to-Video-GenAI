{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Loading AI2D dataset from Hugging Face","metadata":{}},{"cell_type":"markdown","source":"https://huggingface.co/datasets/lmms-lab/ai2d","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\n\nds = load_dataset(\"lmms-lab/ai2d\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-08T07:02:09.042666Z","iopub.execute_input":"2024-11-08T07:02:09.044494Z","iopub.status.idle":"2024-11-08T07:02:14.866591Z","shell.execute_reply.started":"2024-11-08T07:02:09.044380Z","shell.execute_reply":"2024-11-08T07:02:14.864996Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"print(ds)\nprint(ds['test'][8])  ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-08T07:02:14.869386Z","iopub.execute_input":"2024-11-08T07:02:14.870223Z","iopub.status.idle":"2024-11-08T07:02:14.916520Z","shell.execute_reply.started":"2024-11-08T07:02:14.870158Z","shell.execute_reply":"2024-11-08T07:02:14.914462Z"}},"outputs":[{"name":"stdout","text":"DatasetDict({\n    test: Dataset({\n        features: ['question', 'options', 'answer', 'image'],\n        num_rows: 3088\n    })\n})\n{'question': 'What event will probably cause the elephant seal population to increase?', 'options': ['an increase in whales', 'a decrease in penguins', 'a decrease in birds', 'an increase in fish'], 'answer': '3', 'image': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=576x396 at 0x7E8230E65A20>}\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"ds['test'][8]['options'][int(ds['test'][8]['answer'])-1]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-08T07:02:14.918785Z","iopub.execute_input":"2024-11-08T07:02:14.919304Z","iopub.status.idle":"2024-11-08T07:02:14.944715Z","shell.execute_reply.started":"2024-11-08T07:02:14.919255Z","shell.execute_reply":"2024-11-08T07:02:14.943104Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"'a decrease in birds'"},"metadata":{}}],"execution_count":3},{"cell_type":"markdown","source":"### Load CLIP model and processor","metadata":{}},{"cell_type":"code","source":"from transformers import CLIPProcessor, CLIPModel\nfrom datasets import Dataset\n\nprocessor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\nmodel = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-08T07:02:14.946297Z","iopub.execute_input":"2024-11-08T07:02:14.946752Z","iopub.status.idle":"2024-11-08T07:02:33.851715Z","shell.execute_reply.started":"2024-11-08T07:02:14.946708Z","shell.execute_reply":"2024-11-08T07:02:33.850393Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"### Preprocessing the dataset","metadata":{}},{"cell_type":"code","source":"def preprocess_example(example):\n    image = example['image']\n    \n    question = example['question']\n    options = \" \".join(example['options'])  \n    \n    inputs = processor(\n        images=image, \n        text=question + \" \" + options, \n        return_tensors=\"pt\", \n        padding=True,\n        truncation=True,  \n        max_length=77  \n    )\n    \n    label = int(example['answer']) \n    \n    return {\n        'pixel_values': inputs['pixel_values'][0], \n        'input_ids': inputs['input_ids'][0],        \n        'attention_mask': inputs['attention_mask'][0], \n        'labels': label  \n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-08T07:02:33.856262Z","iopub.execute_input":"2024-11-08T07:02:33.857129Z","iopub.status.idle":"2024-11-08T07:02:33.866607Z","shell.execute_reply.started":"2024-11-08T07:02:33.857080Z","shell.execute_reply":"2024-11-08T07:02:33.864919Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"preprocessed_dataset = ds['test'].map(preprocess_example, batched=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-08T07:02:33.868136Z","iopub.execute_input":"2024-11-08T07:02:33.868530Z","iopub.status.idle":"2024-11-08T07:02:33.975913Z","shell.execute_reply.started":"2024-11-08T07:02:33.868491Z","shell.execute_reply":"2024-11-08T07:02:33.974520Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"### Finetuning the CLIP model with AI2D dataset","metadata":{}},{"cell_type":"code","source":"from transformers import TrainingArguments\n\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    num_train_epochs=3,\n    per_device_train_batch_size=2,  # Reduce batch size to lower memory usage\n    evaluation_strategy=\"epoch\",\n    fp16=True,  # Enable mixed precision training if supported\n    logging_dir=\"./logs\",\n    save_total_limit=2,  # Limit the number of checkpoints to save\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-08T07:02:33.977612Z","iopub.execute_input":"2024-11-08T07:02:33.978078Z","iopub.status.idle":"2024-11-08T07:02:35.661948Z","shell.execute_reply.started":"2024-11-08T07:02:33.978031Z","shell.execute_reply":"2024-11-08T07:02:35.660683Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"### Padding","metadata":{}},{"cell_type":"code","source":"from transformers import DataCollatorForSeq2Seq\n\ndata_collator = DataCollatorForSeq2Seq(tokenizer=processor.tokenizer, model=model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-08T07:02:35.663795Z","iopub.execute_input":"2024-11-08T07:02:35.664866Z","iopub.status.idle":"2024-11-08T07:02:35.671418Z","shell.execute_reply.started":"2024-11-08T07:02:35.664814Z","shell.execute_reply":"2024-11-08T07:02:35.670076Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"### We are creating a custom model for the dataset ","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\n\n# Define contrastive loss function\ndef contrastive_loss(logits_per_image, logits_per_text):\n    # We use the cosine similarity to compute the loss\n    labels = torch.arange(logits_per_image.size(0), device=logits_per_image.device)\n    loss_img = F.cross_entropy(logits_per_image, labels)\n    loss_txt = F.cross_entropy(logits_per_text, labels)\n    return (loss_img + loss_txt) / 2\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-08T07:02:35.673122Z","iopub.execute_input":"2024-11-08T07:02:35.673660Z","iopub.status.idle":"2024-11-08T07:02:35.688322Z","shell.execute_reply.started":"2024-11-08T07:02:35.673606Z","shell.execute_reply":"2024-11-08T07:02:35.686735Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"from transformers import Trainer\n\nclass CustomTrainer(Trainer):\n    def training_step(self, model, inputs):\n        # Get the image and text inputs\n        pixel_values = inputs['pixel_values']\n        input_ids = inputs['input_ids']\n        attention_mask = inputs['attention_mask']\n        \n        # Forward pass\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask, pixel_values=pixel_values)\n        \n        # Extract logits for the image and text\n        logits_per_image = outputs.logits_per_image\n        logits_per_text = outputs.logits_per_text\n        \n        # Calculate loss\n        loss = contrastive_loss(logits_per_image, logits_per_text)\n        \n        return loss\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-08T07:02:35.690010Z","iopub.execute_input":"2024-11-08T07:02:35.690442Z","iopub.status.idle":"2024-11-08T07:02:35.719263Z","shell.execute_reply.started":"2024-11-08T07:02:35.690399Z","shell.execute_reply":"2024-11-08T07:02:35.717867Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"import torch\nimport gc\n\ndef clear_memory():\n    gc.collect()\n    torch.cuda.empty_cache()\n\nclear_memory()\n\nper_device_train_batch_size=training_args.per_device_train_batch_size // 2  # or set to a small value, e.g., 2\ngradient_accumulation_steps=4  # Adjust based on memory limits","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-08T07:02:35.721122Z","iopub.execute_input":"2024-11-08T07:02:35.721509Z","iopub.status.idle":"2024-11-08T07:02:36.098353Z","shell.execute_reply.started":"2024-11-08T07:02:35.721470Z","shell.execute_reply":"2024-11-08T07:02:36.097067Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"try:\n    trainer = CustomTrainer(\n        model=model,\n        args=training_args,\n        train_dataset=preprocessed_dataset,\n        data_collator=data_collator\n    )\n    trainer.train()\nexcept RuntimeError as e:\n    if \"CUDA out of memory\" in str(e):\n        print(\"Caught CUDA out of memory error. Reducing batch size or using CPU.\")\n        clear_memory()  # Clear cache if out of memory error occurs\n\n        # Optionally, set no_cuda=True to switch to CPU training\n        training_args.no_cuda = True\n        trainer = CustomTrainer(\n            model=model,\n            args=training_args,\n            train_dataset=preprocessed_dataset,\n            data_collator=data_collator\n        )\n        trainer.train()\n    else:\n        raise e","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-08T07:02:36.100022Z","iopub.execute_input":"2024-11-08T07:02:36.100536Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmshruthi03\u001b[0m (\u001b[33mshruthimohan03\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011113379899999625, max=1.0â€¦","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3ebb5b94b90346518c242783a70ab4ff"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.18.3"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20241108_070241-060gsmfx</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/shruthimohan03/huggingface/runs/060gsmfx' target=\"_blank\">./results</a></strong> to <a href='https://wandb.ai/shruthimohan03/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/shruthimohan03/huggingface' target=\"_blank\">https://wandb.ai/shruthimohan03/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/shruthimohan03/huggingface/runs/060gsmfx' target=\"_blank\">https://wandb.ai/shruthimohan03/huggingface/runs/060gsmfx</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='303' max='4632' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 303/4632 04:00 < 57:31, 1.25 it/s, Epoch 0.20/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}